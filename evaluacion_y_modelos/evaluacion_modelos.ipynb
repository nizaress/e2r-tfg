{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a07d06d",
   "metadata": {},
   "source": [
    "# Generación de los textos facilitados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b55669",
   "metadata": {},
   "source": [
    "### Modelos\n",
    "Usamos los siguientes modelos para generar los textos:\n",
    "- [LLaMA3.1-8B Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n",
    "- [Mistral-8B Instruct](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410)\n",
    "- [Phi-3-Small-128K Instruct](https://huggingface.co/microsoft/Phi-3-small-128k-instruct)\n",
    "- [Qwen2.5-7B Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)\n",
    "- [DeepSeek-R1 Distill Qwen3-8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B)\n",
    "\n",
    "Para realizar las peticiones mediante las APIs utilizo el archivo `config.json`, que contiene los tokens de todos los servicios que utilizo.\n",
    "\n",
    "Por motivos de seguridad este archivo no está incluido en este repositorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04e1272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from mistralai import Mistral\n",
    "from huggingface_hub import InferenceClient\n",
    "import re\n",
    "\n",
    "\n",
    "PROMPT_LECTURA_FACIL = (\n",
    "    \"Eres un experto en accesibilidad y simplificación de textos. Tu tarea es transformar el siguiente texto para que sea comprensible por cualquier persona, incluyendo personas con discapacidad intelectual, dificultades lectoras o bajo nivel de alfabetización.\\n\"\n",
    "    \"Debes aplicar TODAS las pautas de Lectura Fácil de forma estricta y priorizar la claridad, sencillez y accesibilidad. No omitas ni ignores ninguna regla.\\n\"\n",
    "    \"No repitas el texto original ni añadas explicaciones. Responde únicamente con el texto simplificado.\\n\"\n",
    "    \"\\nPAUTAS DE ORTOTIPOGRAFÍA:\\n\"\n",
    "    \"- No uses mayúsculas en palabras o frases completas, excepto en siglas.\\n\"\n",
    "    \"- Usa mayúscula inicial solo al inicio de párrafos, títulos, después de punto o en nombres propios.\\n\"\n",
    "    \"- Separa ideas diferentes con punto y aparte, no con coma.\\n\"\n",
    "    \"- Usa punto y aparte o conjunciones en vez de punto y seguido o coma para ideas relacionadas.\\n\"\n",
    "    \"- Usa dos puntos (:) para listas de más de tres elementos.\\n\"\n",
    "    \"- No uses punto y coma (;).\\n\"\n",
    "    \"- Evita paréntesis, corchetes y signos poco habituales (% & / ...).\\n\"\n",
    "    \"- No uses etcétera ni puntos suspensivos (...), reemplázalos por 'entre otros' o 'y muchos más'.\\n\"\n",
    "    \"- Evita comillas; si las usas, acompáñalas de explicación.\\n\"\n",
    "    \"\\nPAUTAS DE VOCABULARIO:\\n\"\n",
    "    \"- Usa lenguaje sencillo y frecuente, adaptado al público objetivo.\\n\"\n",
    "    \"- Evita términos abstractos, técnicos o complejos.\\n\"\n",
    "    \"- Sustituye palabras homófonas/homógrafas por sinónimos.\\n\"\n",
    "    \"- Evita palabras largas o con sílabas complejas.\\n\"\n",
    "    \"- Evita adverbios terminados en -mente.\\n\"\n",
    "    \"- Evita superlativos, usa 'muy' + adjetivo.\\n\"\n",
    "    \"- Elimina palabras redundantes o innecesarias.\\n\"\n",
    "    \"- Evita palabras en otros idiomas salvo uso común (ej. wifi).\\n\"\n",
    "    \"- No uses abreviaturas ni siglas sin explicar la primera vez.\\n\"\n",
    "    \"- Evita frases nominales y lenguaje figurado (o explícalo).\\n\"\n",
    "    \"- Usa siempre la misma palabra para el mismo referente.\\n\"\n",
    "    \"- Evita palabras indeterminadas (cosa, algo).\\n\"\n",
    "    \"- Escribe los números con cifras; para números grandes, usa comparaciones cualitativas.\\n\"\n",
    "    \"- Separa los dígitos de teléfonos por bloques.\\n\"\n",
    "    \"- Evita números ordinales, usa cardinales.\\n\"\n",
    "    \"- Evita fracciones y porcentajes, usa descripciones equivalentes.\\n\"\n",
    "    \"- Escribe fechas completas (ej. 'el 1 de enero de 2023').\\n\"\n",
    "    \"- Usa el formato de 12 horas con texto (ej. 'las 3 de la tarde').\\n\"\n",
    "    \"- Evita números romanos, escríbelos como se leen.\\n\"\n",
    "    \"\\nPAUTAS DE ORACIONES:\\n\"\n",
    "    \"- Usa frases sencillas, evita oraciones complejas.\\n\"\n",
    "    \"- Usa presente de indicativo siempre que sea posible.\\n\"\n",
    "    \"- Evita tiempos compuestos, condicionales y subjuntivos.\\n\"\n",
    "    \"- Usa voz activa, evita la pasiva y la pasiva refleja.\\n\"\n",
    "    \"- Usa imperativo solo en contextos claros, aclarando a quién se dirige.\\n\"\n",
    "    \"- Evita oraciones impersonales y con gerundio.\\n\"\n",
    "    \"- Evita verbos consecutivos salvo perífrasis con deber, querer, saber o poder.\\n\"\n",
    "    \"- Prefiere oraciones afirmativas, evita doble negación.\\n\"\n",
    "    \"- No uses elipsis, expresa todas las ideas claramente.\\n\"\n",
    "    \"- Evita explicaciones entre comas o aposiciones que corten el ritmo.\\n\"\n",
    "    \"- Limita las oraciones a dos ideas por frase como máximo.\\n\"\n",
    "    \"- Usa conectores simples, evita conectores complejos como 'por lo tanto' o 'sin embargo'.\\n\"\n",
    "    \"\\n\\nTexto: \\n{texto}\\nSimplificación: \"\n",
    ")\n",
    "\n",
    "\n",
    "def get_api_key(model_type, config_file='config.json'):\n",
    "    try:\n",
    "        with open(config_file, 'r') as f:\n",
    "            config = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise RuntimeError(f\"Error: No se encontró el archivo {config_file}\")\n",
    "    except json.JSONDecodeError:\n",
    "        raise RuntimeError(f\"Error: El archivo {config_file} no tiene un formato JSON válido\")\n",
    "    \n",
    "    key_name = f\"{model_type.upper()}_API_KEY\"\n",
    "    api_key = config.get(key_name)\n",
    "    \n",
    "    if not api_key or api_key == f\"your_{model_type.lower()}_api_key_here\":\n",
    "        raise RuntimeError(f\"Error: La API key de {model_type} no esta en {config_file}\")\n",
    "    \n",
    "    return api_key\n",
    "\n",
    "\n",
    "def get_model_client(model_type):\n",
    "    if model_type.lower() == 'mistral':\n",
    "        api_key = get_api_key('mistral')\n",
    "        return Mistral(api_key=api_key)\n",
    "    elif model_type.lower() == 'huggingface':\n",
    "        api_key = get_api_key('huggingface')\n",
    "        return InferenceClient(token=api_key)\n",
    "    else:\n",
    "        raise ValueError(f\"Tipo de modelo no soportado: {model_type}\")\n",
    "\n",
    "\n",
    "def simplificar_texto_mistral(texto_original):\n",
    "    try:\n",
    "        client = get_model_client('mistral')\n",
    "        model = \"mistral-large-latest\"\n",
    "        prompt = PROMPT_LECTURA_FACIL.format(texto=texto_original)\n",
    "        max_tokens = 2 * len(texto_original.split())\n",
    "        response = client.chat.complete(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error en la respuesta del modelo: {str(e)}\"\n",
    "\n",
    "\n",
    "def simplificar_texto_llama3_8b(texto_original):\n",
    "    try:\n",
    "        client = get_model_client('huggingface')\n",
    "        client.provider = \"nebius\"\n",
    "        max_tokens = 2 * len(texto_original.split())\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Eres un asistente experto que ayuda a simplificar textos aplicando correctamente las pautas de Lectura Fácil.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": PROMPT_LECTURA_FACIL.format(texto=texto_original)\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error en la respuesta del modelo Llama3: {str(e)}\"\n",
    "\n",
    "\n",
    "def simplificar_texto_phi(texto_original):\n",
    "    try:\n",
    "        endpoint = \"https://models.github.ai/inference\"\n",
    "        model_name = \"microsoft/Phi-3-small-128k-instruct\"\n",
    "        token = get_api_key('github')\n",
    "        max_tokens = 2 * len(texto_original.split())\n",
    "        client = ChatCompletionsClient(\n",
    "            endpoint=endpoint,\n",
    "            credential=AzureKeyCredential(token),\n",
    "        )\n",
    "        response = client.complete(\n",
    "            messages=[\n",
    "                SystemMessage(\n",
    "                    content=\"Eres un asistente experto que ayuda a simplificar textos aplicando correctamente las pautas de Lectura Fácil.\"\n",
    "                ),\n",
    "                UserMessage(\n",
    "                    content=PROMPT_LECTURA_FACIL.format(texto=texto_original)\n",
    "                ),\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            top_p=1.0,\n",
    "            max_tokens=max_tokens,\n",
    "            model=model_name\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error en la respuesta del modelo Llama3: {str(e)}\"\n",
    "\n",
    "\n",
    "def simplificar_texto_qwen25(texto_original):\n",
    "    try:\n",
    "        client = get_model_client('huggingface')\n",
    "        client.provider = \"together\"\n",
    "        max_tokens = 2 * len(texto_original.split())\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Eres un asistente experto que ayuda a simplificar textos aplicando correctamente las pautas de Lectura Fácil.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": PROMPT_LECTURA_FACIL.format(texto=texto_original)\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error en la respuesta del modelo Qwen: {str(e)}\"\n",
    "\n",
    "\n",
    "def simplificar_texto_deepseek(texto_original):\n",
    "    try:\n",
    "        client = get_model_client('huggingface')\n",
    "        client.provider = \"novita\"\n",
    "        max_tokens = 2 * len(texto_original.split())\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Eres un asistente experto que ayuda a simplificar textos aplicando correctamente las pautas de Lectura Fácil.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": PROMPT_LECTURA_FACIL.format(texto=texto_original)\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        texto = response.choices[0].message.content\n",
    "        texto_filtrado = re.sub(r'<think>[\\s\\S]*?</think>', '', texto)\n",
    "        return texto_filtrado.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error en la respuesta del modelo DeepSeek: {str(e)}\"\n",
    "\n",
    "\n",
    "def simplificar_texto(texto_original, modelo='mistral'):\n",
    "    if modelo.lower() == 'mistral':\n",
    "        return simplificar_texto_mistral(texto_original)\n",
    "    elif modelo.lower() == 'llama3':\n",
    "        return simplificar_texto_llama3_8b(texto_original)\n",
    "    elif modelo.lower() == 'phi':\n",
    "        return simplificar_texto_phi(texto_original)\n",
    "    elif modelo.lower() == 'qwen25':\n",
    "        return simplificar_texto_qwen25(texto_original)\n",
    "    elif modelo.lower() == 'deepseek':\n",
    "        return simplificar_texto_deepseek(texto_original)\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo no soportado: {modelo}\")\n",
    "\n",
    "\n",
    "def procesar_textos_json():\n",
    "    try:\n",
    "        with open('500_facilitadas.json', 'r', encoding='utf-8') as f:\n",
    "            textos = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error leyendo el archivo de entrada: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    resultados = []\n",
    "    try:\n",
    "        for i, entrada in enumerate(textos):\n",
    "            print(f\"Procesando texto {i+1}/{len(textos)}...\")\n",
    "            resultado = {\n",
    "                \"original\": entrada.get(\"TXT\", \"\"),\n",
    "                \"facilitado\": entrada.get(\"FAC\", \"\"),\n",
    "                \"mistral\": simplificar_texto(entrada.get(\"TXT\", \"\"), \"mistral\"),\n",
    "                \"llama3\": simplificar_texto(entrada.get(\"TXT\", \"\"), \"llama3\"),\n",
    "                \"phi\": simplificar_texto(entrada.get(\"TXT\", \"\"), \"phi\"),\n",
    "                \"qwen25\": simplificar_texto(entrada.get(\"TXT\", \"\"), \"qwen25\"),\n",
    "                \"deepseek\": simplificar_texto(entrada.get(\"TXT\", \"\"), \"deepseek\")\n",
    "            }\n",
    "            resultados.append(resultado)\n",
    "    except Exception as e:\n",
    "        print(f\"Error durante el procesamiento: {str(e)}\")\n",
    "    finally:\n",
    "        try:\n",
    "            with open('resultados.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(resultados, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Resultados guardados (parciales o completos) en resultados.json\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Error guardando los resultados: {str(e2)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    procesar_textos_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2220366",
   "metadata": {},
   "source": [
    "# Evaluación de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf0487",
   "metadata": {},
   "source": [
    "### Métricas\n",
    "Para evaluar los modelos, utilizamos las siguientes métricas:\n",
    "- **BLEU**: Mide la similitud entre el texto generado y el texto de referencia.\n",
    "- **ROUGE**: Evalúa la calidad del resumen generado comparándolo con un resumen de referencia.\n",
    "- **METEOR**: Mide la similitud semántica entre el texto generado y el texto de referencia.\n",
    "- **BERTScore**: Utiliza embeddings de BERT para medir la similitud entre el texto generado y el texto de referencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78452652",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce68c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def cargar_resultados(archivo='resultados.json'):\n",
    "    with open(archivo, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def calcular_bleu(texto_generado, texto_referencia):\n",
    "    referencia = word_tokenize(texto_referencia.lower(), language='spanish')\n",
    "    candidato = word_tokenize(texto_generado.lower(), language='spanish')\n",
    "    \n",
    "    return sentence_bleu([referencia], candidato, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "def evaluar_modelos():\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "    \n",
    "    resultados = cargar_resultados()\n",
    "    \n",
    "    scores_por_modelo = {\n",
    "        'mistral': [],\n",
    "        'llama3': [],\n",
    "        'phi': [],\n",
    "        'qwen25': [],\n",
    "        'deepseek': []\n",
    "    }\n",
    "    \n",
    "    for entrada in resultados:\n",
    "        texto_facilitado = entrada['facilitado']\n",
    "        \n",
    "        for modelo in scores_por_modelo.keys():\n",
    "            if modelo in entrada and entrada[modelo] and not entrada[modelo].startswith('Error') and not entrada[modelo].isspace() and not entrada[modelo] == '':\n",
    "                try:\n",
    "                    score = calcular_bleu(entrada[modelo], texto_facilitado)\n",
    "                    scores_por_modelo[modelo].append(score)\n",
    "                except LookupError as e:\n",
    "                    print(f\"Error de tokenización para el modelo {modelo}: {e}\")\n",
    "    \n",
    "    print(\"\\nResultados de evaluación BLEU:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Modelo\\t\\tMedia\\t\\tDesv. Est.\\tMín\\t\\tMáx\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for modelo, scores in scores_por_modelo.items():\n",
    "        if scores:\n",
    "            stats = {\n",
    "                'media': np.mean(scores),\n",
    "                'std': np.std(scores),\n",
    "                'min': np.min(scores),\n",
    "                'max': np.max(scores)\n",
    "            }\n",
    "            print(f\"{modelo:<12}\\t{stats['media']:.4f}\\t\\t{stats['std']:.4f}\\t\\t{stats['min']:.4f}\\t\\t{stats['max']:.4f}\")\n",
    "        else:\n",
    "            print(f\"{modelo:<12}\\tNo hay resultados válidos\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluar_modelos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a41f7b",
   "metadata": {},
   "source": [
    "## ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c299e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def cargar_resultados(archivo='resultados.json'):\n",
    "    with open(archivo, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def calcular_rouge(texto_generado, texto_referencia):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(texto_referencia, texto_generado)\n",
    "    return {\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rouge2': scores['rouge2'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure\n",
    "    }\n",
    "\n",
    "def evaluar_modelos():\n",
    "    resultados = cargar_resultados()\n",
    "    modelos = ['mistral', 'llama3', 'phi', 'qwen25', 'deepseek']\n",
    "    scores_por_modelo = {m: {'rouge1': [], 'rouge2': [], 'rougeL': []} for m in modelos}\n",
    "\n",
    "    for entrada in resultados:\n",
    "        texto_facilitado = entrada['facilitado']\n",
    "        for modelo in modelos:\n",
    "            if modelo in entrada and entrada[modelo] and not entrada[modelo].startswith('Error') and not entrada[modelo].isspace() and not entrada[modelo] == '':\n",
    "                try:\n",
    "                    rouge_scores = calcular_rouge(entrada[modelo], texto_facilitado)\n",
    "                    for k in ['rouge1', 'rouge2', 'rougeL']:\n",
    "                        scores_por_modelo[modelo][k].append(rouge_scores[k])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculando ROUGE para el modelo {modelo}: {e}\")\n",
    "\n",
    "    print(\"\\nResultados de evaluación ROUGE (F1):\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\"Modelo\\t\\tROUGE-1\\t\\tROUGE-2\\t\\tROUGE-L\")\n",
    "    print(\"-\" * 70)\n",
    "    for modelo in modelos:\n",
    "        if scores_por_modelo[modelo]['rouge1']:\n",
    "            r1 = np.mean(scores_por_modelo[modelo]['rouge1'])\n",
    "            r2 = np.mean(scores_por_modelo[modelo]['rouge2'])\n",
    "            rl = np.mean(scores_por_modelo[modelo]['rougeL'])\n",
    "            print(f\"{modelo:<12}\\t{r1:.4f}\\t\\t{r2:.4f}\\t\\t{rl:.4f}\")\n",
    "        else:\n",
    "            print(f\"{modelo:<12}\\tNo hay resultados válidos\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluar_modelos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33ae65",
   "metadata": {},
   "source": [
    "## METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cce64fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/omw-1.4')\n",
    "except LookupError:\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "def cargar_resultados(archivo='resultados.json'):\n",
    "    with open(archivo, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def calcular_meteor(texto_generado, texto_referencia):\n",
    "    try:\n",
    "        referencia_tok = nltk.word_tokenize(texto_referencia, language='spanish')\n",
    "        generado_tok = nltk.word_tokenize(texto_generado, language='spanish')\n",
    "    except:\n",
    "        referencia_tok = nltk.word_tokenize(texto_referencia)\n",
    "        generado_tok = nltk.word_tokenize(texto_generado)\n",
    "    \n",
    "    return meteor_score([referencia_tok], generado_tok)\n",
    "\n",
    "def evaluar_modelos():\n",
    "    resultados = cargar_resultados()\n",
    "    modelos = ['mistral', 'llama3', 'phi', 'qwen25', 'deepseek']\n",
    "    scores_por_modelo = {m: [] for m in modelos}\n",
    "    \n",
    "    for entrada in resultados:\n",
    "        texto_facilitado = entrada['facilitado']\n",
    "        \n",
    "        for modelo in modelos:\n",
    "            if (modelo in entrada and \n",
    "                entrada[modelo] and \n",
    "                not entrada[modelo].startswith('Error') and \n",
    "                not entrada[modelo].isspace() and \n",
    "                not entrada[modelo] == ''):\n",
    "                \n",
    "                try:\n",
    "                    score = calcular_meteor(entrada[modelo], texto_facilitado)\n",
    "                    scores_por_modelo[modelo].append(score)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculando METEOR para el modelo {modelo}: {e}\")\n",
    "    \n",
    "    print(\"\\nResultados de evaluación METEOR:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Modelo\\t\\tMedia\\t\\tDesv. Est.\\tMín\\t\\tMáx\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for modelo, scores in scores_por_modelo.items():\n",
    "        if scores:\n",
    "            stats = {\n",
    "                'media': np.mean(scores),\n",
    "                'std': np.std(scores),\n",
    "                'min': np.min(scores),\n",
    "                'max': np.max(scores)\n",
    "            }\n",
    "            print(f\"{modelo:<12}\\t{stats['media']:.4f}\\t\\t{stats['std']:.4f}\\t\\t{stats['min']:.4f}\\t\\t{stats['max']:.4f}\")\n",
    "        else:\n",
    "            print(f\"{modelo:<12}\\tNo hay resultados válidos\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluar_modelos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63623bb7",
   "metadata": {},
   "source": [
    "## BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e70f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from bert_score import score\n",
    "\n",
    "BERT_MODEL = 'xlm-roberta-base'\n",
    "\n",
    "\n",
    "def cargar_resultados(archivo='resultados.json'):\n",
    "    with open(archivo, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def calcular_bertscore_batch(candidatos, referencias):\n",
    "    P, R, F1 = score(candidatos, referencias, lang='es', model_type=BERT_MODEL, verbose=False)\n",
    "    return F1.tolist()\n",
    "\n",
    "def evaluar_modelos():\n",
    "    resultados = cargar_resultados()\n",
    "    modelos = ['mistral', 'llama3', 'phi', 'qwen25', 'deepseek']\n",
    "    scores_por_modelo = {m: [] for m in modelos}\n",
    "\n",
    "    for modelo in modelos:\n",
    "        candidatos = []\n",
    "        referencias = []\n",
    "        for entrada in resultados:\n",
    "            texto_facilitado = entrada['facilitado']\n",
    "            if modelo in entrada and entrada[modelo] and not entrada[modelo].startswith('Error') and not entrada[modelo].isspace() and not entrada[modelo] == '':\n",
    "                candidatos.append(entrada[modelo])\n",
    "                referencias.append(texto_facilitado)\n",
    "        if candidatos:\n",
    "            try:\n",
    "                f1_scores = calcular_bertscore_batch(candidatos, referencias)\n",
    "                scores_por_modelo[modelo].extend(f1_scores)\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculando BERTScore para el modelo {modelo}: {e}\")\n",
    "\n",
    "    print(\"\\nResultados de evaluación BERTScore (F1):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Modelo\\t\\tMedia\\t\\tDesv. Est.\\tMín\\t\\tMáx\")\n",
    "    print(\"-\" * 50)\n",
    "    for modelo, scores in scores_por_modelo.items():\n",
    "        if scores:\n",
    "            stats = {\n",
    "                'media': np.mean(scores),\n",
    "                'std': np.std(scores),\n",
    "                'min': np.min(scores),\n",
    "                'max': np.max(scores)\n",
    "            }\n",
    "            print(f\"{modelo:<12}\\t{stats['media']:.4f}\\t\\t{stats['std']:.4f}\\t\\t{stats['min']:.4f}\\t\\t{stats['max']:.4f}\")\n",
    "        else:\n",
    "            print(f\"{modelo:<12}\\tNo hay resultados válidos\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluar_modelos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
